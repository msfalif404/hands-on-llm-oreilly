{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:24:01.927362Z","iopub.execute_input":"2025-06-11T05:24:01.927651Z","iopub.status.idle":"2025-06-11T05:24:42.094872Z","shell.execute_reply.started":"2025-06-11T05:24:01.927623Z","shell.execute_reply":"2025-06-11T05:24:42.093991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    device_map=\"cuda\",\n    torch_dtype=\"auto\",\n    trust_remote_code=False\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:24:42.096928Z","iopub.execute_input":"2025-06-11T05:24:42.097467Z","iopub.status.idle":"2025-06-11T05:25:12.138382Z","shell.execute_reply.started":"2025-06-11T05:24:42.097445Z","shell.execute_reply":"2025-06-11T05:25:12.137732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=False,\n    max_new_tokens=500,\n    do_sample=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:25:12.139280Z","iopub.execute_input":"2025-06-11T05:25:12.139609Z","iopub.status.idle":"2025-06-11T05:25:12.145470Z","shell.execute_reply.started":"2025-06-11T05:25:12.139583Z","shell.execute_reply":"2025-06-11T05:25:12.144668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n]\noutput = pipe(messages)\nprint(output[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:25:12.146232Z","iopub.execute_input":"2025-06-11T05:25:12.146522Z","iopub.status.idle":"2025-06-11T05:25:23.242626Z","shell.execute_reply.started":"2025-06-11T05:25:12.146503Z","shell.execute_reply":"2025-06-11T05:25:23.241911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check prompt template\n\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\nprompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:25:23.243546Z","iopub.execute_input":"2025-06-11T05:25:23.243862Z","iopub.status.idle":"2025-06-11T05:25:23.249624Z","shell.execute_reply.started":"2025-06-11T05:25:23.243834Z","shell.execute_reply":"2025-06-11T05:25:23.248975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = pipe(messages, do_sample=True, temperature=1)\nprint(output)\nprint()\nprint(output[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:29:10.145921Z","iopub.execute_input":"2025-06-11T05:29:10.146255Z","iopub.status.idle":"2025-06-11T05:29:12.221757Z","shell.execute_reply.started":"2025-06-11T05:29:10.146232Z","shell.execute_reply":"2025-06-11T05:29:12.220766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = pipe(messages, do_sample=True, top_p=1)\noutput","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:29:20.276962Z","iopub.execute_input":"2025-06-11T05:29:20.277242Z","iopub.status.idle":"2025-06-11T05:29:21.436882Z","shell.execute_reply.started":"2025-06-11T05:29:20.277223Z","shell.execute_reply":"2025-06-11T05:29:21.436176Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prompt Components","metadata":{}},{"cell_type":"code","source":"persona = \"Kamu adalah ahli dalam Large Language Model. Kamu unggul dalam memecah makalah rumit menjadi makalah yang mudah dimengerti.\\n\"\ninstruction = \"Ringkaslah temuan utama dari makalah yang akan diberikan.\\n\"\ncontext = \"Rangkuman yang dibuat harus ada poin yang paling krusial yang bisa membantu peneliti mudah mengerti informasi paling vital dalam makalahnya.\\n\"\ndata_format = \"Buatkan format ringkasannya dalam bentuk urutan poin-poin yang menjelaskan metodenya. Di setiap poinnya terdapat paragraf ringkas yang menjelaskan poin utamanya.\\n\"\naudience = \"Rangkuman ini ditujukan oleh peneliti sibuk yang butuh secara cepat memahami trend terbaru dalam Large Language Model.\\n\"\ntone = \"Nadanya haruslah terlihat professional dan jelas.\\n\"\ntext = \"MY TEXT TO SUMMARIZE.\\n\"\ndata = f\"Text to summarize: {text}\"\n\nquery = persona + instruction + context + data_format + audience + tone + text + data\nquery","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## One-shot Example","metadata":{}},{"cell_type":"code","source":"one_shot_prompt = [\n    {\n        \"role\": \"user\",\n        \"content\": \"A 'Gigamuru' is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:\"\n    },\n    {\n        \"role\": \"system\",\n        \"content\": \"I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is: \"\n    }\n]\n\nprint(tokenizer.apply_chat_template(one_shot_prompt, tokenize=False))\nprint()\n\noutputs = pipe(one_shot_prompt)\nprint(outputs[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T06:24:15.223945Z","iopub.execute_input":"2025-06-11T06:24:15.224403Z","iopub.status.idle":"2025-06-11T06:24:15.228183Z","shell.execute_reply.started":"2025-06-11T06:24:15.224379Z","shell.execute_reply":"2025-06-11T06:24:15.227457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Chain of Instruction","metadata":{}},{"cell_type":"code","source":"product_prompt = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Buatkan nama dan slogan untuk sebuah chatbot yang memanfaatkan LLM.\"\n    }\n]\noutputs = pipe(product_prompt)\nproduct_description = outputs[0][\"generated_text\"]\nproduct_description","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sales_prompt = [\n    {\n        \"role\": \"user\",\n        \"content\": f\"Generate sebuah sales pitch untuk produk ini: {product_description}\"\n    }\n]\noutputs = pipe(sales_prompt)\nsales_pitch = outputs[0][\"generated_text\"]\\\nsales_pitch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Chain of Tought","metadata":{}},{"cell_type":"code","source":"cot_prompt = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Si A punya 5 kelereng. Dia beli 2 bungkus kelereng, setiap bungkusnya ada 3 kelereng. Berapa banyak kelereng yang dia punya sekarang ?\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Awalnya si A punya 5 kelereng. Setiap bungkus kelereng berisi 3 kelereng. Si A beli 2 bungkus. 5 + (3 * 2) = 11.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Si A beli gorengan 3. Dia beli lagi 2 bungkus gorengan, setiap bungkusnya ada 6 gorengan. Berapa gorengan si A sekarang ?\"\n    }\n]\n\noutputs = pipe(cot_prompt)\noutputs[0][\"generated_text\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Zero Shot Chain of Tought","metadata":{}},{"cell_type":"code","source":"zeroshot_cot_prompt = [\n    {\n        \"role\": \"user\",\n        \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let's think step-by-step.\"\n    }\n]\n\noutputs = pipe(zeroshot_cot_prompt)\noutputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Zero Shot Tree of Tought","metadata":{}},{"cell_type":"code","source":"zeroshot_tot_prompt = [\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"\n        Bayangkan ada 3 orang ahli matematika yang berbeda mencoba menjawab 1 pertanyaan.\n        Setiap ahli tersebut haruslah menuliskan langkah per langkah bagaimana dia berpikir untuk\n        menyelesaikan permasalahannya dan harus membagikan jawabannya kepada ahli yang lain.\n        Diakhir para ahli harus berdiskusi dan sepakat mana jawaban yang benar.\n\n        Berikut pertanyaannya:\n        1. Berapakah hasil dari x + 5 = 7\n        \"\"\"\n    }\n]\n\noutputs = pipe(zeroshot_tot_prompt)\noutputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T07:07:44.462992Z","iopub.execute_input":"2025-06-11T07:07:44.463292Z","iopub.status.idle":"2025-06-11T07:07:44.466652Z","shell.execute_reply.started":"2025-06-11T07:07:44.463270Z","shell.execute_reply":"2025-06-11T07:07:44.466069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ouput Verification","metadata":{}},{"cell_type":"code","source":"import gc\nimport torch\n\n# del model, tokenizer, pipe\n\n# Flush Memory\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install llama-cpp-python --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:00:52.223768Z","iopub.execute_input":"2025-06-13T07:00:52.224042Z","iopub.status.idle":"2025-06-13T07:02:52.036707Z","shell.execute_reply.started":"2025-06-13T07:00:52.224016Z","shell.execute_reply":"2025-06-13T07:02:52.035840Z"}},"outputs":[{"name":"stdout","text":"Collecting llama-cpp-python\n  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.2)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.9-cp311-cp311-linux_x86_64.whl size=4127103 sha256=d8b879789be626ed5737431d39bc0371d905786cc79fc992cea4561f8e8edf25\n  Stored in directory: /root/.cache/pip/wheels/9e/8f/bf/148c8eb7d69021eccd6eae6444f3accd48347587054ffd24e5\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.9\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from llama_cpp.llama import Llama\n\nllm = Llama.from_pretrained(\n    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n    filename=\"*fp16.gguf\",\n    n_gpu_layers=-1,\n    n_ctx=2048,\n    verbose=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:02:52.038506Z","iopub.execute_input":"2025-06-13T07:02:52.038771Z","iopub.status.idle":"2025-06-13T07:03:32.899593Z","shell.execute_reply.started":"2025-06-13T07:02:52.038747Z","shell.execute_reply":"2025-06-13T07:03:32.898743Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Phi-3-mini-4k-instruct-fp16.gguf:   0%|          | 0.00/7.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9346f90cd3d34c1b8e1be892ce3c4887"}},"metadata":{}},{"name":"stderr","text":"llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"output = llm.create_chat_completion(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a warrior for an RPG in JSON format.\"}\n    ],\n    response_format={\"type\": \"json_object\"},\n    temperature=0\n)['choices'][0]['message']['content']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:22:29.293887Z","iopub.execute_input":"2025-06-13T07:22:29.294209Z","iopub.status.idle":"2025-06-13T07:26:34.857186Z","shell.execute_reply.started":"2025-06-13T07:22:29.294186Z","shell.execute_reply":"2025-06-13T07:26:34.856561Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import json\n\njson_output = json.dumps(json.loads(output), indent = 4)\nprint(json_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:40:11.164528Z","iopub.execute_input":"2025-06-13T07:40:11.165247Z","iopub.status.idle":"2025-06-13T07:40:11.169754Z","shell.execute_reply.started":"2025-06-13T07:40:11.165221Z","shell.execute_reply":"2025-06-13T07:40:11.169018Z"}},"outputs":[{"name":"stdout","text":"{\n    \"warrior\": {\n        \"name\": \"Eldric Stormbringer\",\n        \"class\": \"Warrior\",\n        \"level\": 5,\n        \"attributes\": {\n            \"strength\": 18,\n            \"dexterity\": 10,\n            \"constitution\": 16,\n            \"intelligence\": 8,\n            \"wisdom\": 10,\n            \"charisma\": 12\n        },\n        \"skills\": [\n            {\n                \"name\": \"Martial Arts\",\n                \"proficiency\": 18\n            },\n            {\n                \"name\": \"Heavy Armor\",\n                \"proficiency\": 16\n            },\n            {\n                \"name\": \"Swordsmanship\",\n                \"proficiency\": 17\n            },\n            {\n                \"name\": \"Shield Mastery\",\n                \"proficiency\": 15\n            },\n            {\n                \"name\": \"Survival\",\n                \"proficiency\": 12\n            }\n        ],\n        \"equipment\": [\n            {\n                \"name\": \"Iron Sword\",\n                \"type\": \"Weapon\",\n                \"damage\": 12,\n                \"durability\": 100\n            },\n            {\n                \"name\": \"Steel Shield\",\n                \"type\": \"Armor\",\n                \"defense\": 15,\n                \"durability\": 100\n            },\n            {\n                \"name\": \"Leather Armor\",\n                \"type\": \"Armor\",\n                \"defense\": 8,\n                \"durability\": 100\n            },\n            {\n                \"name\": \"Healing Potion\",\n                \"type\": \"Consumable\",\n                \"healing_value\": 20\n            }\n        ],\n        \"abilities\": [\n            {\n                \"name\": \"Berserker Rage\",\n                \"description\": \"Increases strength and damage output for a short duration.\",\n                \"cooldown\": 30\n            },\n            {\n                \"name\": \"Shield Bash\",\n                \"description\": \"Stuns an enemy and reduces their defense for a short duration.\",\n                \"cooldown\": 60\n            },\n            {\n                \"name\": \"Endurance\",\n                \"description\": \"Increases constitution and reduces damage taken.\",\n                \"cooldown\": 120\n            }\n        ]\n    }\n}\n","output_type":"stream"}],"execution_count":10}]}